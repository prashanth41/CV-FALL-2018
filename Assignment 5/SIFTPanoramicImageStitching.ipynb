{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT based Image Stitching\n",
    "    \n",
    "Welcome to PA5, in this week we will be studying how to stich multiple images to form a panoramic image. In particular, we will use two images coming from the same scene and will match them by using SIFT features in this assignment in four steps:\n",
    "\n",
    "-  Step 1: First detect keypoints (DoG, Harris, etc.) and extract local invariant descriptors (SIFT, SURF, etc.) from the two input images. We will use features computed by the DoG approach (as they are computed in SIFT).\n",
    "-  Step 2: Then, we will match the computed keypoints between the two images. We will use David Lowe's technique to match the features.\n",
    "-  Step 3: To project one image onto another, we will compute (estimate) the homography matrix. For computing the H matrix, we will use the RANSAC algorithm on our matched feature vectors.\n",
    "-  Step 4: Finally, we will apply a warping transformation using the H matrix obtained in Step 3 to combine both images.\n",
    "\n",
    "In this assignment you'll use OpenCV, an essential library for many computer vision applications in Python.\n",
    "\n",
    "## Prerequisite Packages\n",
    "\n",
    "You may need to install the OpenCV with the following commands \n",
    "\n",
    "    pip install opencv-python==3.4.2.16\n",
    "    pip install opencv-contrib-python==3.4.2.16\n",
    "    pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the three pip commands below to install openCV related libraries if you do not have the necessary openCV libraries in your system!\n",
    "\n",
    "# You can run those 3 pip commands in your console seperately too before starting the jupyter notebook!\n",
    "\n",
    "# If you are using Google Colab to run this notebook, then set the following variable useGoogleColab = 1 \n",
    "# If you do not need to install those openCV libraries, then set: useGoogleColab = 0\n",
    "useGoogleColab = 0\n",
    "\n",
    "if useGoogleColab == 1:\n",
    "    !pip install opencv-python==3.4.2.16\n",
    "    !pip install opencv-contrib-python==3.4.2.16\n",
    "    !pip install imutils\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "First, lets have a look at two images that we will use in this assignment: \n",
    "    \n",
    "\n",
    "Image 1 | Image 2\n",
    ":------------------------:|:-----------------------:\n",
    "<img src=\"images/bryce_left_02.png\" style=\"width:200px;height:150px;\">  |  <img src=\"images/bryce_right_02.png\" style=\"width:200px;height:150px;\">\n",
    "\n",
    "The final result should look like this image:\n",
    "\n",
    "Panorama |\n",
    ":------------------------:|\n",
    "<img src=\"images/result.png\" style=\"width:350px;height:150px;\"> |\n",
    "\n",
    "### Topics Covered\n",
    "Lets first visualize those images on our own system with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us read the input images first.\n",
    "#if you are using Google Colab to run this notebook, then set the following variable useGoogleColab = 1 \n",
    "useGoogleColab = 1\n",
    "if useGoogleColab == 0:\n",
    "    image1 = cv2.imread(\"./images/bryce_left_02.png\")\n",
    "    image2 = cv2.imread(\"./images/bryce_right_02.png\")\n",
    "else:\n",
    "    !wget \"https://github.com/Rice-Field/SIFT-Panorama-Stitch/blob/master/images/bryce_left_02.png?raw=1\"\n",
    "    !wget \"https://github.com/Rice-Field/SIFT-Panorama-Stitch/blob/master/images/bryce_right_02.png?raw=1\"\n",
    "    image1 = cv2.imread(\"bryce_left_02.png?raw=1\")\n",
    "    image2 = cv2.imread(\"bryce_right_02.png?raw=1\")\n",
    "\n",
    "print(\"Original image dimension: \" + str(image1.shape))\n",
    "\n",
    "# We keep the images small to keep the computations short\n",
    "image1 = imutils.resize(image1, width=400)\n",
    "image2 = imutils.resize(image2, width=400)\n",
    "\n",
    "print(\"Resized image dimension: \" + str(image1.shape))\n",
    "\n",
    "# OpenCV loads images in BGR format. Lets convert that to our RGB format that we are more comfortable with.\n",
    "image1_RGB = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(image1_RGB)\n",
    "plt.show()\n",
    "\n",
    "image2_RGB = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(image2_RGB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Key Points\n",
    "\n",
    "Now that we can read and plot both images in this Jupyter notebook, we should start with initializing a SIFT descriptor object. In openCV, we already have a default function that initializes the SIFT object. Using that SIFT object, we can compute the keypoints and their 128 dimensional features (also known as descriptors). After that, we convert the CV keypoint objects into numpy arrays for visualization and for further processing. Each sift keypoint has a 128 dimensional feature vector. In other words, each key point is described with 128 dimensional vector (Each keypoint is represented locally with its 4 x 4 neighbor blocks where each block is represented by 8 dimensional histograms, thus 4x4x8=128).See the lecture slides for further details.\n",
    "\n",
    "| Difference of Gaussian    | SIFT Descriptor       |                \n",
    "|:------------------------:|:-----------------------:|\n",
    "| <img src=\"figs/dog.PNG\" style=\"width:200px;height:150px;\"> | <img src=\"figs/descriptor.PNG\" style=\"width:300px;height:150px;\"> |\n",
    "| | [source](https://opencv-python-tutroals.readthedocs.io) |\n",
    "\n",
    "\n",
    "**Functions:**\n",
    "We will use the following functions that come under openCV in this exercise:\n",
    "    \n",
    "    cv2.xfeatures2d.SIFT_create()\n",
    "    object.detectAndCompute(image, None)\n",
    "    cv2.drawKeypoints(input_image, keypoints, output_image)\n",
    "\n",
    "<div class=\"alert alert-info\"><h1>Exercise 1</h1>\n",
    "    <p> use OpenCV's sift computing function to compute the keypoints (<code>kps1</code> and <code>kps2</code>) and their descriptors (<code>features1</code> and <code>features2</code> ) for both images below. Then plot those keypoints on each image. </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DoG and SIFT\n",
    "descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Extract features from each image\n",
    "#----- code below -------\n",
    "# ~ one line of code\n",
    "(kps1, features1) = None\n",
    "#~ one line of code\n",
    "(kps2, features2) = None\n",
    "#---- end of code -------\n",
    "\n",
    "# Show the detected keypoints on both images\n",
    "draw1 = image1.copy()\n",
    "cv2.drawKeypoints(draw1, kps1, draw1)\n",
    "draw1_RGB = cv2.cvtColor(draw1, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(draw1_RGB)\n",
    "plt.show()\n",
    "\n",
    "draw2 = image2.copy()\n",
    "cv2.drawKeypoints(draw2, kps2, draw2)\n",
    "draw2_RGB = cv2.cvtColor(draw2, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(draw2_RGB)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# convert the keypoint types from openCV's KeyPoint objects to NumPy arrays\n",
    "kps1 = np.float32([kp.pt for kp in kps1])\n",
    "kps2 = np.float32([kp.pt for kp in kps2])\n",
    "\n",
    "print(\"Feature descriptor length: \" + str(len(features1[0])))\n",
    "print(\"Keypoint length: \" + str(len(kps1[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected ouput:**\n",
    "\n",
    "|Draw 1 | Draw 2 |             \n",
    "|:------------------------:|:-----------------------:|\n",
    "| <img src=\"images/draw1.png\" style=\"width:200px;height:150px;\"> | <img src=\"images/draw2.png\" style=\"width:200px;height:150px;\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matching Points\n",
    "\n",
    "Now that we have keypoints and their descriptors for each image computed, we need to find the keypoint pairs that exist in both images. For that purpose, we initialize a object matcher with \"Brute Force\". This parameter ensures that each feature from image1 is compared to all the features from the second image. We then use the k-nearest neighbor algorithm, with k = 2, to find the top two matches for each points.\n",
    "\n",
    "We then apply Lowe's ratio test to select matches where the best match is a ratio better than the second best.\n",
    "\n",
    "**Functions:**\n",
    "\n",
    "We will use the following functions that come under openCV in this exercise:\n",
    "    \n",
    "    cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "    object.knnMatch(features A, features B, k)\n",
    "    \n",
    "    \n",
    "    \n",
    "<div class=\"alert alert-info\"><h1>Exercise 2</h1>\n",
    "    <p> use openCV to compute the matching ke-ypoint pairs (<code>initialMatches</code>) between both images below. </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the raw matches and initialize the list of actual matches\n",
    "matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "\n",
    "#------ write your code below -------\n",
    "# ~one line of code\n",
    "initialMatches = None\n",
    "#------- end of your code ---------\n",
    "\n",
    "matches = []\n",
    "\n",
    "ratio=0.75\n",
    "# loop over the intial matches and apply Lowe's ratio test to see if they are good matches.\n",
    "for m in initialMatches:\n",
    "    # check to see if the distance is within a certain ratio of each\n",
    "    # other (i.e. Lowe's ratio test)\n",
    "    if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "        matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: No output here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Homography Matrix: \n",
    "\n",
    "With our computed keypoint matches, we can now compute the Homography (H) matrix necessary to project one image onto the other. We will use RANSAC algorithm to compute the H matrix. \n",
    "\n",
    "|[Source](https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html) |          \n",
    "|:------------------------:|\n",
    "| <img src=\"figs/homography.jpg\" style=\"width:500px;height:150px;\"> |\n",
    "\n",
    "**Functions:**\n",
    "\n",
    "    cv2.findHomography(pointset1, pointset2, cv2.RANSAC, Ransac threshold)\n",
    "    cv2.circle(image, points, radius, (B,G,R), thickness)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h1>Exercise 3</h1>\n",
    "    <p> use openCV to compute Homography matrix H (<code>(H, status)</code>) by using the RANSAC algorithm. Then, plot only the matching keypoints on each image individually (<code>match1</code> and <code>match2</code>). </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing a homography requires at least 4 matches\n",
    "# use the ransacThresh value given below to compute the H matrix within RANSAC\n",
    "if len(matches) > 4:\n",
    "    # construct the two sets of keypoints as np arrays\n",
    "    ransacThresh=4.0\n",
    "    pts1 = np.float32([kps1[i] for (_, i) in matches])\n",
    "    pts2 = np.float32([kps2[i] for (i, _) in matches])\n",
    "\n",
    "    # compute the homography between the two sets of points by using RANSAC\n",
    "    #------ write your code below -------\n",
    "    # ~ one line of code\n",
    "    (H, status) = None\n",
    "    #------ end of your code -------\n",
    "\n",
    "match1 = image1.copy()\n",
    "match2 = image2.copy()\n",
    "# loop over the matches\n",
    "for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "    # only process the match if the keypoint was successfully\n",
    "    # matched\n",
    "    if s == 1:\n",
    "        # draw the match\n",
    "        pt1 = (int(kps1[queryIdx][0]), int(kps1[queryIdx][1]))\n",
    "        pt2 = (int(kps2[trainIdx][0]), int(kps2[trainIdx][1]))\n",
    "        \n",
    "        # use the color red for circles\n",
    "        #------ write your code below -------\n",
    "        # ~one line of code \n",
    "        match1 = None\n",
    "        # ~one line of code\n",
    "        match2 = None\n",
    "        #------ end of your code -------\n",
    "        \n",
    "match1_RGB = cv2.cvtColor(match1, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(match1_RGB)\n",
    "plt.show()\n",
    "\n",
    "match2_RGB = cv2.cvtColor(match2, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(match2_RGB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected ouput:**\n",
    "\n",
    "|Draw 1 | Draw 2 |             \n",
    "|:------------------------:|:-----------------------:|\n",
    "| <img src=\"images/match1.png\" style=\"width:200px;height:150px;\"> | <img src=\"images/match2.png\" style=\"width:200px;height:150px;\"> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of only the matching key-points\n",
    "\n",
    "Now that we know what the stiched image looks like, lets have a look at what features matched in both images. For, that, we will visualize the matching features on both images shown side by side. For that we will pick all the matching key point-pairs and visualize them pair by pair as a line. In this way, the left side of the image will show the first image, and the right hand side will show the second image. Since we need two points to draw a line, all we need is actually those individual matching pairs to draw matching lines. See the expected results below. You should see mostly parallel lines.\n",
    "\n",
    "**Functions:**\n",
    "\n",
    "    cv2.line(image, point 1, point 2, (0, 255, 0), thicknessVal)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h1>Exercise 4</h1>\n",
    "    <p> use openCV to plot the new image containing the matching keypoints only  (<code>initialMatches</code>) between both images below. </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the output visualization image\n",
    "(h1, w1) = image1.shape[:2]\n",
    "(h2, w2) = image2.shape[:2]\n",
    "vis = np.zeros((max(h1, h2), w1 + w2, 3), dtype=\"uint8\")\n",
    "vis[0:h1, 0:w1] = match1\n",
    "vis[0:h2, w1:] = match2\n",
    "\n",
    "# loop over the matches\n",
    "for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "    # only process the match if the keypoint was successfully\n",
    "    # matched\n",
    "    if s == 1:\n",
    "        # draw the match\n",
    "        pt1 = (int(kps1[queryIdx][0]), int(kps1[queryIdx][1]))\n",
    "        pt2 = (int(kps2[trainIdx][0]) + w1, int(kps2[trainIdx][1]))\n",
    "\n",
    "        # use the color green for lines\n",
    "        #------ write your code below -------\n",
    "        vis = None\n",
    "        # ------ end of your code -----------\n",
    "        \n",
    "vis_RGB = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(vis_RGB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected ouput:**\n",
    "\n",
    "|Draw 1 |          \n",
    "|:------------------------:|\n",
    "| <img src=\"images/vis.png\" style=\"width:400px;height:150px;\"> |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Warp and Stitch\n",
    "\n",
    "Finally after computing the homography matrix, we warp the second image (called image1) on to the first one (called image2). Meanwhile, we also ensure that the size of the final (stiched) image can fit the both images to be stiched. \n",
    "\n",
    "**Functions:** You can use openCV's built-in function warpPerspective here (see below).\n",
    "\n",
    "    cv2.warpPerspective(image2, HomographyMatrix,(image2_width + image1_width, image2_height))\n",
    "    \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h1>Exercise 5</h1>\n",
    "    <p> Given the computed H matrix and both images, use openCV to stitch both images into a single image  by projecting one image onto the other one and obtain the final stiched image (<code>result</code>) below. </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (matches, H, status) = M\n",
    "#------ write your code below -------\n",
    "# ~ one line of code\n",
    "result = None\n",
    "#------ end of your code -------\n",
    "\n",
    "result[0:image1.shape[0], 0:image1.shape[1]] = image1\n",
    "        \n",
    "#---- Now visualize the final stiched image ---\n",
    "result_RGB = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(result_RGB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected ouput:**\n",
    "\n",
    "|Draw 1 |          \n",
    "|:------------------------:|\n",
    "| <img src=\"images/result.png\" style=\"width:400px;height:150px;\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to compute SIFT features, how to match them and how to stich images! You also practiced how to compute the homogprahy matrix by using RANSAC now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
